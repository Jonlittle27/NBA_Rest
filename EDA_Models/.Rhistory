#checking for categorical variables in Adult data
glimpse(mydata)
#subset the data for only categorical varibales and target variable income
cate <- mydata[c("workclass","education","marital.status", "occupation",
"relationship","race","sex", "native.country",
"income")]
#Q25/A
#Workclass didn't see a strong relationship in most outcomes, but
#the private class showed a massive outlier in the <=50k income bracket
count <- cate %>%
group_by(income)%>%
count(workclass)
ggplot(count, aes(x = workclass, y = n, fill = income)) +
geom_col(position = "dodge") +
ylab("Count")
#education shows a correlation with income over 50k even in this bar
#graph because as the education goes up the count <=50k vastly grows
count <- cate %>%
group_by(income)%>%
count(education)
ggplot(count, aes(x = education, y = n, fill = income)) +
geom_col(position = "dodge")+
ylab("Count")
#Only never-married and divorced show any real difference between
#the income brackets, but never married is a major difference
count <- cate %>%
group_by(income)%>%
count(marital.status)
ggplot(count, aes(x = marital.status, y = n, fill = income)) +
geom_col(position = "dodge")+
ylab("Count")
#Occupation surprisingly doesn't do much to explain which income
#bracket a data point would be in because all of them are drastically
#in the greater than 50k income value
count <- cate %>%
group_by(income)%>%
count(occupation)
ggplot(count, aes(x = occupation, y = n, fill = income)) +
geom_col(position = "dodge")+
ylab("Count")
#Once again there is not a very strong connection between relationship and income
#every value besides wife is strongly in favor of <=50k
count <- cate %>%
group_by(income)%>%
count(relationship)
ggplot(count, aes(x = relationship, y = n, fill = income)) +
geom_col(position = "dodge")+
ylab("Count")
#This variable on the surface looks like it would show a strong relationship
#between being white and income greater than or equal to 50k, but
#the other values don't strongly suggest that not being white bring income down
count <- cate %>%
group_by(income)%>%
count(race)
ggplot(count, aes(x = race, y = n, fill = income)) +
geom_col(position = "dodge")+
ylab("Count")
#Sex shows nothing because both values are drastically in favor of <=50k
count <- cate %>%
group_by(income)%>%
count(sex)
ggplot(count, aes(x = sex, y = n, fill = income)) +
geom_col(position = "dodge")+
ylab("Count")
#Native country is probably the worst variable comparison because a
#negligible amount of values come from outside the U.S.
count <- cate %>%
group_by(income)%>%
count(native.country)
ggplot(count, aes(x = native.country, y = n, fill = income)) +
geom_col(position = "dodge")+
ylab("Count")
#Q25/B
#I expect education to be the most useful during a classification model
#and I wouldn't expect the rest to be particularly viable for modelling
#Q28
#In multiple variables there are anomalous fields and values.  In Q26,
#there is plenty of proof showing value counts of just income devianting
#from the mean drastically.  One example would be in workclass,
#where the private workclass has well over 15000 counts of <=50k and every
#other class at its highest is near 2500.
#Q29
#subsetting numerical data
num <- select(mydata,-c("workclass","education","marital.status", "occupation",
"relationship","race","sex", "native.country",
"income"))
#mode function
mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
#summary statistics function
sum_stats <- function(x){
list(mean(x),median(x),mode(x))
}
#applying the sum_stats function to the variables and saving
summary <- sapply(num,sum_stats)%>%
data.frame()
#changing row names
rownames(summary) <- c("mean","median", "mode")
#changing to matrix to print report to console
as.matrix(summary)
summary(mydata)
for(i in length(cate)) {
crosstab(cate, row.vars = c(cate[i],cate[i+1]), col.vars = cate$income, type = "j")
}
source("http://pcwww.liv.ac.uk/~william/R/crosstab.r")
for(i in length(cate)) {
crosstab(cate, row.vars = c(cate[i],cate[i+1]), col.vars = cate$income, type = "j")
}
for(i in length(cate)) {
crosstab(cate, row.vars = c(cate[i,],cate[i+1,]), col.vars = cate$income, type = "j")
}
for(i in length(cate)) {
crosstab(cate, row.vars = c(cate[,i],cate[,i+1]), col.vars = cate$income, type = "j")
}
crosstab(mydata, row.vars = c("workclass", "sex"), col.vars = "income", type = "j")
crosstab(mydata, row.vars = c("education", "occupation"), col.vars = "income", type = "j")
#crosstab for
crosstab(mydata, row.vars = c("occuptation", "education"), col.vars = "income", type = "j")
#crosstab for
crosstab(mydata, row.vars = c("occupation", "education"), col.vars = "income", type = "j")
View(cate)
#crosstab for
crosstab(mydata, row.vars = c("workclass", "education"), col.vars = "income", type = "j")
#crosstab for
crosstab(mydata, row.vars = c("marital.status", "relationship"), col.vars = "income", type = "j")
count <- cate %>%
group_by(income)%>%
count(education)
ggplot(count, aes(x = education, y = n, fill = income)) +
geom_col(position = "dodge")+
ylab("Count")
count <- cate %>%
group_by(income)%>%
count(race)
ggplot(count, aes(x = race, y = n, fill = income)) +
geom_col(position = "dodge")+
ylab("Count")
#changing to matrix to print report to console
as.matrix(summary)
xtabs(~ education + occupation + income, data = mydata)
xtabs(~ occupation + education + income, data = mydata)
crosstab(mydata, row.vars = c("occupation", "education"), col.vars = "income", type = "j")
#crosstab for
crosstab(mydata, row.vars = c("sex", "relationship"), col.vars = "income", type = "j")
count <- cate %>%
group_by(income)%>%
count(workclass)
ggplot(count, aes(x = workclass, y = n, fill = income)) +
geom_col(position = "dodge") +
ylab("Count")
count <- cate %>%
group_by(income)%>%
count(education)
ggplot(count, aes(x = education, y = n, fill = income)) +
geom_col(position = "dodge")+
ylab("Count")
#Only never-married and divorced show any real difference between
#the income brackets, but never married is a major difference
count <- cate %>%
group_by(income)%>%
count(marital.status)
ggplot(count, aes(x = marital.status, y = n, fill = income)) +
geom_col(position = "dodge")+
ylab("Count")
count <- cate %>%
group_by(income)%>%
count(occupation)
ggplot(count, aes(x = occupation, y = n, fill = income)) +
geom_col(position = "dodge")+
ylab("Count")
count <- cate %>%
group_by(income)%>%
count(relationship)
ggplot(count, aes(x = relationship, y = n, fill = income)) +
geom_col(position = "dodge")+
ylab("Count")
#the other values don't strongly suggest that not being white brings income down
count <- cate %>%
group_by(income)%>%
count(race)
ggplot(count, aes(x = race, y = n, fill = income)) +
geom_col(position = "dodge")+
ylab("Count")
count <- cate %>%
group_by(income)%>%
count(sex)
ggplot(count, aes(x = sex, y = n, fill = income)) +
geom_col(position = "dodge")+
ylab("Count")
as.matrix(summary)
#crosstab for workclass/sex and income
crosstab(mydata, row.vars = c("workclass", "sex"), col.vars = "income", type = "j")
#crosstab for sex/relationship
crosstab(mydata, row.vars = c("sex", "relationship"), col.vars = "income", type = "j")
#crosstab for occupation/education and income
crosstab(mydata, row.vars = c("occupation", "education"), col.vars = "income", type = "j")
#crosstab for occupation/education and income
crosstab(mydata, row.vars = c("education", "occupation"), col.vars = "income", type = "j")
#crosstab for occupation/education and income
crosstab(mydata, row.vars = c("occupation", "education"), col.vars = "income", type = "j")
#crosstab for occupation/education and income
crosstab(mydata, row.vars = c("education", "occupation"), col.vars = "income", type = "j")
#Mod 3 Part 1
#libraries
library(tidyverse)
#load in adult data
mydata <- read.csv(url("https://learn-us-east-1-prod-fleet01-xythos.s3.amazonaws.com/5c1802599953e/65332998?response-cache-control=private%2C%20max-age%3D21600&response-content-disposition=inline%3B%20filename%2A%3DUTF-8%27%27Adult.txt&response-content-type=text%2Fplain&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200917T150000Z&X-Amz-SignedHeaders=host&X-Amz-Expires=21600&X-Amz-Credential=AKIAZH6WM4PL5SJBSTP6%2F20200917%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=bc15a724ac9ec23d8981cf96683247500ec5c21c181a7dcef00515ee73c3059d"))
#load in crosstab function
source("http://pcwww.liv.ac.uk/~william/R/crosstab.r")
#crosstab for workclass/sex and income
crosstab(mydata, row.vars = c("workclass", "sex"), col.vars = "income", type = "j")
#this data clearly has more male respondents than female and in all workclasses male has a higher value count.  However, in all cases, besides self-emp-not-inc, Males are 3-5 times to be in the >50k bracket despite being relatively closer in total sum.
crosstab(mydata, row.vars = c("education", "occupation"), col.vars = "income", type = "j")
#I think these results are very messy and can be hard to read with so many values in both variables, but there is also a lot of information that can be pulled.  For example, it shows that a HS grad's best chance at earning >50k is in craft-repairs.  Furthermore, it shows that both doctorate and masters level respondents are more likely to be in the >50k bracket.  For these post-bachelor's level respondents the vast majority of >50k success lies in either the Exec-managerial occupation or Prof-specialty.
library(tidyverse)
professions_df <- data.frame(prestige = c(57, 56, 53, 52, 52, 46, 40, 28, 24, 20, 18, 16), professions = c("Firefighter", "Scientist", "Doctor", "Nurse", "Teacher", "Military Officer", "Clergy", "Congressman", "Lawyer", "Athlete", "Journalist", "Actor") )professionsdfmod <- mutate(highlight_teacher = ifelse(professions_df$professions == 'Teacher', T, F), professions_df)
ggplot(professionsdfmod, aes(reorder(professions, prestige), prestige, fill=highlight_teacher==TRUE)) + geom_col() + labs(title = "Comparing Prestige of Professions", caption = "Harris Poll, July 2008") + xlab("Professions") + ylab("% of respondents who attribute 'very great prestige'" ) + theme(legend.position = "none", panel.grid.major = element_blank(), panel.grid.minor = element_blank(),) + coord_flip()+ scale_fill_grey(start=0.6, end=0.2)
professions_df <- data.frame(prestige = c(57, 56, 53, 52, 52, 46, 40, 28, 24, 20, 18, 16), professions = c("Firefighter", "Scientist", "Doctor", "Nurse", "Teacher", "Military Officer", "Clergy", "Congressman", "Lawyer", "Athlete", "Journalist", "Actor") )
professionsdfmod <- mutate(highlight_teacher = ifelse(professions_df$professions == 'Teacher', T, F), professions_df)
ggplot(professionsdfmod, aes(reorder(professions, prestige), prestige, fill=highlight_teacher==TRUE)) + geom_col() + labs(title = "Comparing Prestige of Professions", caption = "Harris Poll, July 2008") + xlab("Professions") + ylab("% of respondents who attribute 'very great prestige'" ) + theme(legend.position = "none", panel.grid.major = element_blank(), panel.grid.minor = element_blank(),) + coord_flip()+ scale_fill_grey(start=0.6, end=0.2)
ggplot(professionsdfmod, aes(reorder(professions, prestige), prestige, fill=highlight_teacher==TRUE))
+ geom_col()
+ labs(title = "Comparing Prestige of Professions", caption = "Harris Poll, July 2008")
+ xlab("Professions")
+ ylab("% of respondents who attribute 'very great prestige'" )
+ theme(legend.position = "none", panel.grid.major = element_blank(), panel.grid.minor = element_blank(),)
+ coord_flip()
+ scale_fill_grey(start=0.6, end=0.2)
+ scale_x_continuous(breaks = seq(0,60,10))
ggplot(professionsdfmod, aes(reorder(professions, prestige), prestige, fill=highlight_teacher==TRUE)) + geom_col() + labs(title = "Comparing Prestige of Professions", caption = "Harris Poll, July 2008") + xlab("Professions") + ylab("% of respondents who attribute 'very great prestige'" ) + theme(legend.position = "none", panel.grid.major = element_blank(), panel.grid.minor = element_blank(),) + coord_flip()+ scale_fill_grey(start=0.6, end=0.2)+ scale_x_continuous(breaks = seq(0,60,10))
ggplot(professionsdfmod, aes(reorder(professions, prestige), prestige, fill=highlight_teacher==TRUE)) + geom_col() + labs(title = "Comparing Prestige of Professions", caption = "Harris Poll, July 2008") + xlab("Professions") + ylab("% of respondents who attribute 'very great prestige'" ) + theme(legend.position = "none", panel.grid.major = element_blank(), panel.grid.minor = element_blank(),) + coord_flip()+ scale_fill_grey(start=0.6, end=0.2)+ scale_x_discrete(breaks = seq(0,60,10))
ggplot(professionsdfmod, aes(reorder(professions, prestige), prestige, fill=highlight_teacher==TRUE)) + geom_col() + labs(title = "Comparing Prestige of Professions", caption = "Harris Poll, July 2008") + xlab("Professions") + ylab("% of respondents who attribute 'very great prestige'" ) + theme(legend.position = "none", panel.grid.major = element_blank(), panel.grid.minor = element_blank(),) + coord_flip()+ scale_fill_grey(start=0.6, end=0.2)
ggplot(professionsdfmod, aes(reorder(professions, prestige), prestige, fill=highlight_teacher==TRUE)) + geom_col() + labs(title = "Comparing Prestige of Professions", caption = "Harris Poll, July 2008") + xlab("Professions") + ylab("% of respondents who attribute 'very great prestige'" ) + theme(legend.position = "none", panel.grid.major = element_blank(), panel.grid.minor = element_blank(),) + coord_flip()+ scale_fill_grey(start=0.6, end=0.2)+ scale_y_continuous(breaks = seq(0,60,10))
ggplot(professionsdfmod, aes(reorder(professions, prestige), prestige, fill=highlight_teacher==TRUE)) + geom_col() + labs(title = "Comparing Prestige of Professions", caption = "Harris Poll, July 2008") + xlab("Professions") + ylab("% of respondents who attribute 'very great prestige'" ) + theme(legend.position = "none", panel.grid.major = element_blank(), panel.grid.minor = element_blank(),) + coord_flip()+ scale_fill_grey(start=0.6, end=0.2)+ scale_y_continuous(breaks = seq(0,60,5))
ggplot(professionsdfmod, aes(reorder(professions, prestige), prestige, fill=highlight_teacher==TRUE)) + geom_col() + labs(title = "Comparing Prestige of Professions", caption = "Harris Poll, July 2008") + xlab("Professions") + ylab("% of respondents who attribute 'very great prestige'" ) + theme(legend.position = "none", panel.grid.major = element_blank(), panel.grid.minor = element_blank(),) + coord_flip()+ scale_fill_grey(start=0.6, end=0.2)+ scale_y_continuous(breaks = seq(0,60,10))
ggplot(professionsdfmod, aes(reorder(professions, prestige), prestige, fill=highlight_teacher==TRUE)) + geom_col() + labs(title = "Comparing Prestige of Professions", caption = "Harris Poll, July 2008") + xlab("Professions") + ylab("% of respondents who attribute 'very great prestige'" ) + theme(legend.position = "none", panel.grid.major = element_blank(), panel.grid.minor = element_blank(),) + coord_flip()+ scale_fill_grey(start=0.6, end=0.2)+ scale_y_continuous(breaks = seq(0,60,5))
#libraries
library(tidyverse)
setwd("~/Git/NBA_Rest/EDA_Models")
#data import
bucks_16_20_rest <- read_csv("~/Git/NBA_Rest/data/bucks_16_20_rest.csv")
View(bucks_16_20_rest)
#data import
bucks_16_20_rest <- read.csv("~/Git/NBA_Rest/data/bucks_16_20_rest.csv"
, stringsAsFactors = T)
View(bucks_16_20_rest)
colnames(bucks_16_20_rest)
#CART fit
cartfit <- rpart(results ~ locations + rest + game_nums,
data = bucks_16_20_rest,
method = "class")
library(rpart)
#CART fit
cartfit <- rpart(results ~ locations + rest + game_nums,
data = bucks_16_20_rest,
method = "class")
summary(cartfit)
#evalaute
#cart eval
eval_cart <- predict(cartfit, bucks_16_20_rest, type = "class")
cart_cm <- confusionMatrix(eval_cart,bucks_16_20_rest$results)
library(C50)
library(caret)
cart_cm <- confusionMatrix(eval_cart,bucks_16_20_rest$results)
cart_cm
#putting variables in x and y
x <- bucks_16_20_rest[,c(3,4,6)]
y <- bucks_16_20_rest$results
#c50 fit
c50fit <-C5.0(x,y)
#c50 eval
eval_c50 <- predict(c50fit, norm_train, type = "class")
c50_cm <- confusionMatrix(eval_c50,norm_train$Survived)
summary(c50fit)
#c50 fit
c50fit <-C5.0(x,y)
#c50 eval
eval_c50 <- predict(c50fit, bucks_16_20_rest, type = "class")
c50_cm <- confusionMatrix(eval_c50,bucks_16_20_rest$results)
summary(c50fit)
c50_cm
#libraries
library(tidyverse)
library(rpart)
library(rpart.plot)
library(C50)
library(neuralnet)
library(caret)
library(e1071)
setwd("~/Grad School/Data Mining/final/scripts")
#data import from prep
train_data <- read.csv("../data/new_train_data.csv", stringsAsFactors = T)
test_data <- read.csv("../data/new_test_data.csv", stringsAsFactors = T)
#predicting survived variable which is already encoded
#normalize numeric data
normalize <- function(x) { ((x-min(x))/(max(x)-min(x)))}
norm_train <- train_data
norm_test  <- test_data
norm_train$Age <- normalize(norm_train$Age)
norm_train$Fare <- normalize(norm_train$Fare)
norm_train$Age <- normalize(norm_train$Age)
norm_train$Parch <- normalize(norm_train$Parch)
norm_train$SibSp <- normalize(norm_train$SibSp)
norm_test$Age <- normalize(norm_test$Age)
norm_test$Fare <- normalize(norm_test$Fare)
norm_test$Age <- normalize(norm_test$Age)
norm_test$Parch <- normalize(norm_test$Parch)
norm_test$SibSp <- normalize(norm_test$SibSp)
#first method - Neural Network
#encoding pclass
norm_train$Pclass_1 <- norm_train$Pclass_2 <- norm_train$Pclass_3 <- c(rep(0, length(norm_train$Pclass)))
norm_test$Pclass_1 <- norm_test$Pclass_2 <- norm_test$Pclass_3 <- c(rep(0, length(norm_test$Pclass)))
for(i in 1:length(norm_train$Pclass)) {
if(norm_train$Pclass[i] == "1") {norm_train$Pclass_1[i] <- 1}
else if(norm_train$Pclass[i] == "2") {norm_train$Pclass_2[i] <- 1}
else if(norm_train$Pclass[i] == "3") {norm_train$Pclass_3[i] <- 1}
}
for(i in 1:length(norm_test$Pclass)) {
if(norm_test$Pclass[i] == "1") {norm_test$Pclass_1[i] <- 1}
else if(norm_test$Pclass[i] == "2") {norm_test$Pclass_2[i] <- 1}
else if(norm_test$Pclass[i] == "3") {norm_test$Pclass_3[i] <- 1}
}
#train nn
NNfit <- neuralnet(formula = Survived ~ Age + SibSp + Parch +
Embarked_S + Embarked_Q + Embarked_C + Fare +
Sex_f + Sex_m + Pclass_1 + Pclass_2 + Pclass_1,
data = norm_train, rep = 2, hidden = 2, linear.output = F)
#nn eval
nn_train_test <- norm_train[,c(6,7,8,10,13,14,15,16,17,18,19,20)]
nn_pred <- predict(NNfit, nn_train_test, type = "class")
nn_pred[nn_pred < 0.7] <- 0
nn_pred[nn_pred >= 0.7] <- 1
nn_cm <- confusionMatrix(as.factor(nn_pred), as.factor(norm_train$Survived))
#NN prediction on test
NNtest <- norm_test[,c(5,6,7,9,12,13,14,15,16,17,18,19)]
NNpred <- predict(NNfit, NNtest, type = "class")
NNpred[NNpred < 0.7] <- 0
NNpred[NNpred >= 0.7] <- 1
#passenger name and prediction data frame
NN_info <- data.frame("Name" = norm_test$Name, "Survival.Predictions" = NNpred)
View(NN_info)
setwd("~/Git/NBA_Rest/EDA_Models")
#data import
bucks_df <- read.csv("~/Git/NBA_Rest/data/bucks_16_20_rest_dist.csv"
, stringsAsFactors = T)
#CART model
View(bucks_df)
#CART model
#CART fit
cartfit <- rpart(results ~ locations + rest + game_nums +
opponent + travel_miles,
data = bucks_df,
method = "class")
summary(cartfit)
#cart eval
summary(cartfit)
eval_cart <- predict(cartfit, bucks_df, type = "class")
cart_cm <- confusionMatrix(eval_cart,bucks_df$results)
cart_cm
#libraries
library(tidyverse)
library(rpart)
library(C50)
library(caret)
#data import
bucks_df <- read.csv("~/Git/NBA_Rest/data/bucks_16_20_rest_dist.csv"
, stringsAsFactors = T)
#CART model
#CART fit
cartfit <- rpart(results ~ locations + rest + game_nums +
opponent + game_arenas + travel_miles,
data = bucks_df,
method = "class")
#cart eval
summary(cartfit)
eval_cart <- predict(cartfit, bucks_df, type = "class")
cart_cm <- confusionMatrix(eval_cart,bucks_df$results)
cart_cm
#c50 model
#putting variables in x and y
x <- bucks_df[,c(3,4,5,7,8,9)]
y <- bucks_df$results
eval_c50 <- predict(c50fit, bucks_df, type = "class")
c50_cm <- confusionMatrix(eval_c50,bucks_df$results)
#c50 fit
c50fit <-C5.0(x,y)
#c50 eval
eval_c50 <- predict(c50fit, bucks_df, type = "class")
c50_cm <- confusionMatrix(eval_c50,bucks_df$results)
#c50 model
#putting variables in x and y
x <- bucks_df[,c(3,4,5,7,9)]
y <- bucks_df$results
#c50 fit
c50fit <-C5.0(x,y)
eval_c50 <- predict(c50fit, bucks_df, type = "class")
c50_cm <- confusionMatrix(eval_c50,bucks_df$results)
c50_cm
cart_cm
#CART model
#CART fit
cartfit <- rpart(results ~ locations + rest + game_nums +
opponent + travel_miles,
data = bucks_df,
method = "class")
#cart eval
summary(cartfit)
eval_cart <- predict(cartfit, bucks_df, type = "class")
cart_cm <- confusionMatrix(eval_cart,bucks_df$results)
cart_cm
#c50 model
#putting variables in x and y
x <- bucks_df[,c(3,4,5,7,9)]
y <- bucks_df$results
#c50 fit
c50fit <-C5.0(x,y)
#c50 eval
eval_c50 <- predict(c50fit, bucks_df, type = "class")
c50_cm <- confusionMatrix(eval_c50,bucks_df$results)
c50_cm
#CART fit
cartfit <- rpart(results ~ locations + rest  +
opponent + travel_miles,
data = bucks_df,
method = "class")
#cart eval
summary(cartfit)
eval_cart <- predict(cartfit, bucks_df, type = "class")
cart_cm <- confusionMatrix(eval_cart,bucks_df$results)
cart_cm
#CART fit
cartfit <- rpart(results ~ locations + rest + game_nums +
opponent + travel_miles,
data = bucks_df,
method = "class")
#cart eval
summary(cartfit)
eval_cart <- predict(cartfit, bucks_df, type = "class")
cart_cm <- confusionMatrix(eval_cart,bucks_df$results)
cart_cm
dmy <- dummyVars(" ~ .", data = bucks_df)
trsf <- data.frame(predict(dmy, newdata = bucks_df))
View(trsf)
bucks_df <- bucks_df[,-c(1,8)]
dmy <- dummyVars(" ~ .", data = bucks_df)
trsf <- data.frame(predict(dmy, newdata = bucks_df))
View(trsf)
View(dmy)
nn_df <- data.frame(predict(dmy, newdata = bucks_df))
View(nn_df)
#neural net
nn_df <- bucks_df
View(nn_df)
dmy <- dummyVars(" ~ .", data = bucks_df)
nn_df <- data.frame(predict(dmy, newdata = bucks_df))
View(nn_df)
colnames(nn_df)
NN_fit <- neuralnet(formula = results.Win ~ locations.Away + locaitons.Home +
rest + game_nums + date_diff + opponent.ATL +
opponent.BOS + opponent.BRK + opponent.CHI + opponent.CHO +
opponent.CLE + opponent.DAL + opponent.DEN + opponent.DET +
opponent.GSW + opponent.HOU + opponent.IND + opponent.LAC +
opponent.LAL + opponent.MEM + opponent.MIA + opponent.MIN +
opponent.NOP + opponent.NYK + opponent.OKC + opponent.ORL +
opponent.PHI + opponent.PHO + opponent.POR + opponent.SAC +
opponent.SAS + opponent.TOR + opponent.UTA + opponent.WAS +
travel_miles,
data = nn_df, rep = 2, hidden = 2, linear.output = F)
NN_fit <- neuralnet(formula = results.Win ~ locations.Away + locaitons.Home +
rest + game_nums + date_diff + opponent.ATL +
opponent.BOS + opponent.BRK + opponent.CHI + opponent.CHO +
opponent.CLE + opponent.DAL + opponent.DEN + opponent.DET +
opponent.GSW + opponent.HOU + opponent.IND + opponent.LAC +
opponent.LAL + opponent.MEM + opponent.MIA + opponent.MIN +
opponent.NOP + opponent.NYK + opponent.OKC + opponent.ORL +
opponent.PHI + opponent.PHO + opponent.POR + opponent.SAC +
opponent.SAS + opponent.TOR + opponent.UTA + opponent.WAS +
travel_miles,
data = nn_df, rep = 2, hidden = 2, linear.output = F)
NN_fit <- neuralnet(formula = results.Win ~ locaitons.Home +
rest + game_nums + date_diff + opponent.ATL +
opponent.BOS + opponent.BRK + opponent.CHI + opponent.CHO +
opponent.CLE + opponent.DAL + opponent.DEN + opponent.DET +
opponent.GSW + opponent.HOU + opponent.IND + opponent.LAC +
opponent.LAL + opponent.MEM + opponent.MIA + opponent.MIN +
opponent.NOP + opponent.NYK + opponent.OKC + opponent.ORL +
opponent.PHI + opponent.PHO + opponent.POR + opponent.SAC +
opponent.SAS + opponent.TOR + opponent.UTA + opponent.WAS +
travel_miles,
data = nn_df, rep = 2, hidden = 2, linear.output = F)
colnames(nn_df)
NN_fit <- neuralnet(formula = results.Win ~ locations.Away + locations.Home +
rest + game_nums + date_diff + opponent.ATL +
opponent.BOS + opponent.BRK + opponent.CHI + opponent.CHO +
opponent.CLE + opponent.DAL + opponent.DEN + opponent.DET +
opponent.GSW + opponent.HOU + opponent.IND + opponent.LAC +
opponent.LAL + opponent.MEM + opponent.MIA + opponent.MIN +
opponent.NOP + opponent.NYK + opponent.OKC + opponent.ORL +
opponent.PHI + opponent.PHO + opponent.POR + opponent.SAC +
opponent.SAS + opponent.TOR + opponent.UTA + opponent.WAS +
travel_miles,
data = nn_df, rep = 2, hidden = 2, linear.output = F)
#predicting wins (wins are 1s losses are 0s)
nn_df_test <- nn_df[,-c(1,2)]
nn_df_test <- nn_df[,-c(1,2)]
nn_pred <- predict(NN_fit, nn_df_test, type = "class")
nn_pred[nn_pred < 0.7] <- 0
nn_pred[nn_pred >= 0.7] <- 1
nn_cm <- confusionMatrix(as.factor(nn_pred), as.factor(nn_df$results.Win))
nn_cm
